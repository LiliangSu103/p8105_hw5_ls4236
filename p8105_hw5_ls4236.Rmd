---
title: "p8105_hw5_ls4236"
author: "Liliang Su"
date: "2025-10-30"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(patchwork)
library(ggridges)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Write a function to check if there is at least two people sharing a birthday.

```{r}
# source("./source/bday_sim.R")

bday_sim = function(n_room){
  
  birthdays = sample(1:365, n_room, replace = TRUE)
  
  repeated_bday = length(unique(birthdays)) < n_room
  
  repeated_bday
}

```

Iterate the function and Store the results.

```{r}
bday_sim_results =
  expand_grid(
    bdays = 2:50,
    iter = 1:10000
  ) |> 
  mutate(
    result = map_lgl(bdays, bday_sim)
  ) |> 
  group_by(bdays) |> 
  summarise(
    prob_repeat = mean(result)
  )
```

Plot the probability as a function of group size.

```{r}
bday_sim_results |> 
  ggplot(aes(x = bdays, y = prob_repeat)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Duplicate Birthday Probablity", 
    x = "Probablity", 
    y = "Group size"
    )
```

## Problem 2

### Simulation

Fix the sample size `n` and standard deviation `sigma` of distribution.

```{r}
n = 30 # fixed sample size
sigma = 5 # true standard deviation
```

Other variables.

```{r}
n_sim = 5000 # number of datasets to generate for each mu
mu_null = 0 # null hypothesis value
alpha = 0.05 # significance level
mu_values = c(1, 2, 3, 4, 5, 6) # true mean values awaiting to test
```

Write the function to conduct t-test for a single sample

```{r}
power_sim = function(mu_true, n_subj = 30, sigma_true = 5, mu_null = 0, alpha = 0.05){
  
  # generate the data
  x = rnorm(n = n_subj, mean = mu_true, sd = sigma_true)
  
  # perform the one-sample t-test
  test_output = t.test(x, mu = mu_null, conf.level = alpha)
  
  # clean output and calculate results
  test_output_cleaned = broom::tidy(test_output) |>
    # select the point estimate (mu_hat) and p-value
    select(mu_hat = estimate, p_value = p.value) |>
    mutate(
      # determine if H0 was rejected
      rejected_H0 = (p_value < alpha)
    )
  
  test_output_cleaned
}

```

Test the function for true `mu` = 0 iteratively.

```{r}
sim_results_mean0 = 
  expand_grid(
    mu_true = 0,
    iter = 1:n_sim
  ) |> 
  mutate(
    results = map(mu_true, power_sim)
  ) |> 
  unnest(results)

sim_results_mean0 |> 
  head(10) |> 
  knitr::kable()

```


Create a data frame to store simulation results of all other conditions.

```{r}
power_sim_results_df = 
  expand_grid(
    mu_true = mu_values,
    iter = 1:n_sim
  ) |> 
  mutate(
    results = map(mu_true, power_sim)
  ) |> 
  unnest(results)

power_sim_results_df |> 
  head(10) |> 
  knitr::kable()

```

### Plotting

```{r}
# Calculate the power by averaging across the 5000 simulation runs
power_df = power_sim_results_df |>
  group_by(mu_true) |>
  summarise(
    power = mean(rejected_H0), 
    .groups = 'drop'
  )

# Plot 1: proportion of times the null was rejected vs. true mean values
power_plot = power_df |>
  ggplot(aes(x = mu_true, y = power)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "red") +
  labs(
    title = "Plot 1: Power vs. True Mean at 0.05 Significance level",
    x = "True Mean",
    y = "Power (Proportion of Rejections)",
    caption = paste0("n=", n, ", σ=", sigma, ", α=", alpha)
  ) 

power_plot
```

Association bewteen effect size and power:

- The plot clearly shows a strong positive association between the true effect size ($\mu_{true} - \mu_0$) and the power of the test. In other words, as the true mean increases from 1 to 6, the difference between the null value ($\mu_0 = 0$) and the true mean increases. 

- The relatively larger standardized effect size ($\frac{\mu_{true} - \mu_0}{\sigma}$) also makes it easier for the $t$-test to detect the effect, thus increasing the probability of correctly rejecting the false null hypothesis.


```{r}
# Calculate mean estimates under two different conditions
mean_estimates_df = power_sim_results_df |>
  group_by(mu_true) |>
  summarise(
    # mean estimate across all samples
    mu_hat_all = mean(mu_hat),
    # mean estimate only across samples where H0 was rejected
    mu_hat_rejected = mean(mu_hat[rejected_H0]),
    .groups = 'drop'
  ) |>
  # Reshape data for plotting
  pivot_longer(
    -mu_true, 
    names_to = "group", 
    values_to = "avg_estimate"
  ) |>
  mutate(
    group = factor(group, 
                   levels = c("mu_hat_all", "mu_hat_rejected"),
                   labels = c("Average Estimate (All Samples)", 
                              "Average Estimate (Rejected H0 only)")
    )
  )

# Plot 2: average estimate vs. true mean (two overlaid plots)
estimate_plot = mean_estimates_df |>
  ggplot(aes(x = mu_true, y = avg_estimate, color = group, group = group)) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  labs(
    title = "Plot 2: Average Estimate vs. True Mean",
    x = "True Mean",
    y = "Average Estimate of Mean",
    color = "Sample Group"
  )

estimate_plot
```

Discussion on Bias by the overlaid plot:

Is the average estimate of $\hat{\mu}$ across rejected tests approximately equal to the true mean $\mu_{true}$?

- No. For estimates that Rejected H0, the average estimate is significantly greater than the true mean $\mu_{true}$, especially for smaller effect sizes ($\mu=1, 2, 3$), which is referred to as "selection bias", though it gradually overlaps with average estimate across all samples (close to $\mu_{true}$) as the effect size increases.

- For estimates of ALL samples, due to the Central Limit Theorem, when sample size is large enough, the average estimate across all 5000 simulated samples is, as expected, approximately equal to the true mean.


Why the Rejected Samples are Biased?

- When the true effect size ($\mu_{true}$) is small, the test has low power. Hence for the null hypothesis to be rejected under low power, the random sample must have produced an extremely large estimate ($\hat{\mu}$) that is much further from the null ($\mu=0$) than the true mean ($\mu_{true}$).

- As the true effect size and power increase, the bias decreases because in most samples, the null hypothesis are rejected anyway, and selection is less restrictive.

