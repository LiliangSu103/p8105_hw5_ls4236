p8105_hw5_ls4236
================
Liliang Su
2025-10-30

## Problem 1

Write a function to check if there is at least two people sharing a
birthday.

``` r
# source("./source/bday_sim.R")

bday_sim = function(n_room){
  
  birthdays = sample(1:365, n_room, replace = TRUE)
  
  repeated_bday = length(unique(birthdays)) < n_room
  
  repeated_bday
}
```

Iterate the function and Store the results.

``` r
bday_sim_results =
  expand_grid(
    bdays = 2:50,
    iter = 1:10000
  ) |> 
  mutate(
    result = map_lgl(bdays, bday_sim)
  ) |> 
  group_by(bdays) |> 
  summarise(
    prob_repeat = mean(result)
  )
```

Plot the probability as a function of group size.

``` r
bday_sim_results |> 
  ggplot(aes(x = bdays, y = prob_repeat)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Duplicate Birthday Probablity", 
    x = "Probablity", 
    y = "Group size"
    )
```

<img src="p8105_hw5_ls4236_files/figure-gfm/unnamed-chunk-3-1.png" width="90%" />

## Problem 2

### Simulation

Fix the sample size `n` and standard deviation `sigma` of distribution.

``` r
n = 30 # fixed sample size
sigma = 5 # true standard deviation
```

Other variables.

``` r
n_sim = 5000 # number of datasets to generate for each mu
mu_null = 0 # null hypothesis value
alpha = 0.05 # significance level
mu_values = c(1, 2, 3, 4, 5, 6) # true mean values awaiting to test
```

Write the function to conduct t-test for a single sample

``` r
t_test_sim = function(mu_true, n_subj = 30, sigma_true = 5, mu_null = 0, alpha = 0.05){
  
  # generate the data
  x = rnorm(n = n_subj, mean = mu_true, sd = sigma_true)
  
  # perform the one-sample t-test
  test_output = t.test(x, mu = mu_null, conf.level = alpha)
  
  # clean output and calculate results
  test_output_cleaned = broom::tidy(test_output) |>
    # select the point estimate (mu_hat) and p-value
    select(mu_hat = estimate, p_value = p.value) |>
    mutate(
      # determine if H0 was rejected
      rejected_H0 = (p_value < alpha)
    )
  
  test_output_cleaned
}
```

Test the function for true `mu` = 0 iteratively.

``` r
sim_results_mean0 = 
  expand_grid(
    mu_true = 0,
    iter = 1:n_sim
  ) |> 
  mutate(
    results = map(mu_true, t_test_sim)
  ) |> 
  unnest(results)

sim_results_mean0 |> 
  head(10) |> 
  knitr::kable()
```

| mu_true | iter |     mu_hat |   p_value | rejected_H0 |
|--------:|-----:|-----------:|----------:|:------------|
|       0 |    1 |  0.1184308 | 0.8977428 | FALSE       |
|       0 |    2 | -0.8178995 | 0.3694241 | FALSE       |
|       0 |    3 |  2.1001656 | 0.0240196 | TRUE        |
|       0 |    4 | -0.5546838 | 0.6060560 | FALSE       |
|       0 |    5 | -0.0886055 | 0.9181967 | FALSE       |
|       0 |    6 |  0.5875644 | 0.4690264 | FALSE       |
|       0 |    7 | -0.3454635 | 0.6975385 | FALSE       |
|       0 |    8 | -0.4716839 | 0.5893218 | FALSE       |
|       0 |    9 |  0.5943155 | 0.4143503 | FALSE       |
|       0 |   10 | -0.6333851 | 0.5293971 | FALSE       |

Create a data frame to store simulation results of all other conditions.

``` r
power_sim_results_df = 
  expand_grid(
    mu_true = mu_values,
    iter = 1:n_sim
  ) |> 
  mutate(
    results = map(mu_true, t_test_sim)
  ) |> 
  unnest(results)

power_sim_results_df |> 
  head(10) |> 
  knitr::kable()
```

| mu_true | iter |     mu_hat |   p_value | rejected_H0 |
|--------:|-----:|-----------:|----------:|:------------|
|       1 |    1 | -1.0439116 | 0.1476593 | FALSE       |
|       1 |    2 |  1.9463990 | 0.0417558 | TRUE        |
|       1 |    3 |  1.7790620 | 0.0525960 | FALSE       |
|       1 |    4 |  1.2824013 | 0.1488340 | FALSE       |
|       1 |    5 |  1.4638948 | 0.1783545 | FALSE       |
|       1 |    6 |  0.0172275 | 0.9862114 | FALSE       |
|       1 |    7 |  1.4564820 | 0.0894072 | FALSE       |
|       1 |    8 |  2.8956871 | 0.0044960 | TRUE        |
|       1 |    9 |  0.6095836 | 0.5870893 | FALSE       |
|       1 |   10 |  1.7927243 | 0.0866110 | FALSE       |

### Plotting

``` r
# Calculate the power by averaging across the 5000 simulation runs
power_df = power_sim_results_df |>
  group_by(mu_true) |>
  summarise(
    power = mean(rejected_H0), 
    .groups = 'drop'
  )

# Plot 1: proportion of times the null was rejected vs. true mean values
power_plot = power_df |>
  ggplot(aes(x = mu_true, y = power)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "red") +
  labs(
    title = "Plot 1: Power vs. True Mean at 0.05 Significance level",
    x = "True Mean",
    y = "Power (Proportion of Rejections)",
    caption = paste0("n=", n, ", σ=", sigma, ", α=", alpha)
  ) 

power_plot
```

<img src="p8105_hw5_ls4236_files/figure-gfm/unnamed-chunk-9-1.png" width="90%" />

Association bewteen effect size and power:

- The plot clearly shows a strong positive association between the true
  effect size ($\mu_{true} - \mu_0$) and the power of the test. In other
  words, as the true mean increases from 1 to 6, the difference between
  the null value ($\mu_0 = 0$) and the true mean increases.

- The relatively larger standardized effect size
  ($\frac{\mu_{true} - \mu_0}{\sigma}$) also makes it easier for the
  $t$-test to detect the effect, thus increasing the probability of
  correctly rejecting the false null hypothesis.

``` r
# Calculate mean estimates under two different conditions
mean_estimates_df = power_sim_results_df |>
  group_by(mu_true) |>
  summarise(
    # mean estimate across all samples
    mu_hat_all = mean(mu_hat),
    # mean estimate only across samples where H0 was rejected
    mu_hat_rejected = mean(mu_hat[rejected_H0]),
    .groups = 'drop'
  ) |>
  # Reshape data for plotting
  pivot_longer(
    -mu_true, 
    names_to = "group", 
    values_to = "avg_estimate"
  ) |>
  mutate(
    group = factor(group, 
                   levels = c("mu_hat_all", "mu_hat_rejected"),
                   labels = c("Average Estimate (All Samples)", 
                              "Average Estimate (Rejected H0 only)")
    )
  )

# Plot 2: average estimate vs. true mean (two overlaid plots)
estimate_plot = mean_estimates_df |>
  ggplot(aes(x = mu_true, y = avg_estimate, color = group, group = group)) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  labs(
    title = "Plot 2: Average Estimate vs. True Mean",
    x = "True Mean",
    y = "Average Estimate of Mean",
    color = "Sample Group"
  )

estimate_plot
```

<img src="p8105_hw5_ls4236_files/figure-gfm/unnamed-chunk-10-1.png" width="90%" />

Discussion on Bias by the overlaid plot:

Is the average estimate of $\hat{\mu}$ across rejected tests
approximately equal to the true mean $\mu_{true}$?

- No. For estimates that Rejected H0, the average estimate is
  significantly greater than the true mean $\mu_{true}$, especially for
  smaller effect sizes ($\mu=1, 2, 3$), which is referred to as
  “selection bias”, though it gradually overlaps with average estimate
  across all samples (close to $\mu_{true}$) as the effect size
  increases.

- For estimates of ALL samples, due to the Central Limit Theorem, when
  sample size is large enough, the average estimate across all 5000
  simulated samples is, as expected, approximately equal to the true
  mean.

Why the Rejected Samples are Biased?

- When the true effect size ($\mu_{true}$) is small, the test has low
  power. Hence for the null hypothesis to be rejected under low power,
  the random sample must have produced an extremely large estimate
  ($\hat{\mu}$) that is much further from the null ($\mu=0$) than the
  true mean ($\mu_{true}$).

- As the true effect size and power increase, the bias decreases because
  in most samples, the null hypothesis are rejected anyway, and
  selection is less restrictive.

## Problem 3

### Import and describe raw data

``` r
homi_df = 
  read_csv(file = "./data/homicide-data.csv", na = c("NA",".","")) |> 
  janitor::clean_names()
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

This dataset contains 52179 observations and 12 variables detailing
homicides across various U.S. cities.

Here are some key variables:

- `uid` (`character`): Unquie identifier for each homicide case.

- `reported_date` (`numeric`): Variable that stores the exact time the
  homicide was reported.

- Victim Demographics:

  - `victim_last`, `victim_first` (`character`): Victim’s name.
  - `victim_race` (`character`), `victim_age` (`character`),
    `victim_sex` (`character`) : Victims’ detail information.

- Location:

  - `city` (`character`), `state` (`character`), `lat` (`numeric`),
    `lon` (`numeric`): Variables specify the location of the homicide.

- Case Status:

  - `disposition` (`character`): Variable indicates the final status of
    the case.

### Tidy up data

``` r
homi_summary_df = homi_df |> 
  # create the new city_state variable
  mutate(
    city_state = str_c(city, ", ", state)
  ) |> 
  # summarize to get total and unsolved homicides
  group_by(city_state) |> 
  summarise(
    total_homicides = n(),
    unsolved_homicides = sum(
      disposition %in% c("Closed without arrest", "Open/No arrest")
    ),
    .groups = 'drop'
  )
```

### Test on Baltimore, MD

``` r
baltimore_df = homi_summary_df |> 
  filter(city_state == "Baltimore, MD")

x = pull(baltimore_df, unsolved_homicides)
n = pull(baltimore_df, total_homicides)

# save the test output as an object and tidy up
baltimore_prop_test = prop.test(x = x, n = n)

output_cleaned = broom::tidy(baltimore_prop_test)

# pull the estimated proportion and confidence intervals
baltimore_results = output_cleaned |> 
  select(
    p_hat = estimate, 
    conf_low = conf.low, 
    conf_high = conf.high
  )

baltimore_results |> 
  knitr::kable()
```

|     p_hat |  conf_low | conf_high |
|----------:|----------:|----------:|
| 0.6455607 | 0.6275625 | 0.6631599 |

### Write proportion test function

``` r
# Function
prop_test = function(dataset) {
  
  n_success = pull(dataset, unsolved_homicides)
  n_total = pull(dataset, total_homicides)
  
  test_output = prop.test(x = n_success, n = n_total)

  test_output_cleaned = broom::tidy(test_output) |> 
  select(
    p_hat = estimate, 
    conf_low = conf.low, 
    conf_high = conf.high
    )
  
  test_output_cleaned
  
}

# Tidy Pipeline
results_all_df = homi_summary_df |> 
  group_by(city_state) |> 
  # Nest into a new list-column 'city_data'
  nest(city_data = c(total_homicides, unsolved_homicides)) |> 
  mutate(
    results = map(city_data, prop_test)
  ) |> 
  select(-city_data) |> 
  unnest(results)
```

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `results = map(city_data, prop_test)`.
    ## ℹ In group 49: `city_state = "Tulsa, AL"`.
    ## Caused by warning in `prop.test()`:
    ## ! Chi-squared approximation may be incorrect

``` r
results_all_df |> 
  knitr::kable()
```

| city_state         |     p_hat |  conf_low | conf_high |
|:-------------------|----------:|----------:|----------:|
| Albuquerque, NM    | 0.3862434 | 0.3372604 | 0.4375766 |
| Atlanta, GA        | 0.3833505 | 0.3528119 | 0.4148219 |
| Baltimore, MD      | 0.6455607 | 0.6275625 | 0.6631599 |
| Baton Rouge, LA    | 0.4622642 | 0.4141987 | 0.5110240 |
| Birmingham, AL     | 0.4337500 | 0.3991889 | 0.4689557 |
| Boston, MA         | 0.5048860 | 0.4646219 | 0.5450881 |
| Buffalo, NY        | 0.6122841 | 0.5687990 | 0.6540879 |
| Charlotte, NC      | 0.2998544 | 0.2660820 | 0.3358999 |
| Chicago, IL        | 0.7358627 | 0.7239959 | 0.7473998 |
| Cincinnati, OH     | 0.4452450 | 0.4079606 | 0.4831439 |
| Columbus, OH       | 0.5304428 | 0.5002167 | 0.5604506 |
| Dallas, TX         | 0.4811742 | 0.4561942 | 0.5062475 |
| Denver, CO         | 0.5416667 | 0.4846098 | 0.5976807 |
| Detroit, MI        | 0.5883287 | 0.5687903 | 0.6075953 |
| Durham, NC         | 0.3659420 | 0.3095874 | 0.4260936 |
| Fort Worth, TX     | 0.4644809 | 0.4222542 | 0.5072119 |
| Fresno, CA         | 0.3470226 | 0.3051013 | 0.3913963 |
| Houston, TX        | 0.5074779 | 0.4892447 | 0.5256914 |
| Indianapolis, IN   | 0.4493192 | 0.4223156 | 0.4766207 |
| Jacksonville, FL   | 0.5111301 | 0.4820460 | 0.5401402 |
| Kansas City, MO    | 0.4084034 | 0.3803996 | 0.4370054 |
| Las Vegas, NV      | 0.4141926 | 0.3881284 | 0.4407395 |
| Long Beach, CA     | 0.4126984 | 0.3629026 | 0.4642973 |
| Los Angeles, CA    | 0.4900310 | 0.4692208 | 0.5108754 |
| Louisville, KY     | 0.4531250 | 0.4120609 | 0.4948235 |
| Memphis, TN        | 0.3190225 | 0.2957047 | 0.3432691 |
| Miami, FL          | 0.6048387 | 0.5685783 | 0.6400015 |
| Milwaukee, wI      | 0.3614350 | 0.3333172 | 0.3905194 |
| Minneapolis, MN    | 0.5109290 | 0.4585150 | 0.5631099 |
| Nashville, TN      | 0.3624511 | 0.3285592 | 0.3977401 |
| New Orleans, LA    | 0.6485356 | 0.6231048 | 0.6731615 |
| New York, NY       | 0.3875598 | 0.3494421 | 0.4270755 |
| Oakland, CA        | 0.5364308 | 0.5040588 | 0.5685037 |
| Oklahoma City, OK  | 0.4851190 | 0.4467861 | 0.5236245 |
| Omaha, NE          | 0.4132029 | 0.3653146 | 0.4627477 |
| Philadelphia, PA   | 0.4478103 | 0.4300380 | 0.4657157 |
| Phoenix, AZ        | 0.5514223 | 0.5184825 | 0.5839244 |
| Pittsburgh, PA     | 0.5340729 | 0.4942706 | 0.5734545 |
| Richmond, VA       | 0.2634033 | 0.2228571 | 0.3082658 |
| Sacramento, CA     | 0.3696809 | 0.3211559 | 0.4209131 |
| San Antonio, TX    | 0.4285714 | 0.3947772 | 0.4630331 |
| San Bernardino, CA | 0.6181818 | 0.5576628 | 0.6753422 |
| San Diego, CA      | 0.3796095 | 0.3354259 | 0.4258315 |
| San Francisco, CA  | 0.5067873 | 0.4680516 | 0.5454433 |
| Savannah, GA       | 0.4674797 | 0.4041252 | 0.5318665 |
| St. Louis, MO      | 0.5396541 | 0.5154369 | 0.5636879 |
| Stockton, CA       | 0.5990991 | 0.5517145 | 0.6447418 |
| Tampa, FL          | 0.4567308 | 0.3881009 | 0.5269851 |
| Tulsa, AL          | 0.0000000 | 0.0000000 | 0.9453792 |
| Tulsa, OK          | 0.3310463 | 0.2932349 | 0.3711192 |
| Washington, DC     | 0.4379182 | 0.4112495 | 0.4649455 |

### Plotting

``` r
unsolved_proportions_plot = results_all_df |>
  ggplot(
    aes(y = fct_reorder(city_state, p_hat), x = p_hat, xmin = conf_low, xmax = conf_high)
         ) +
  # Add the confidence intervals as horizontal error bars
  geom_errorbarh(height = 0.2, color = "black") +
  # Add the point estimates
  geom_point(color = "dodgerblue", size = 1.5) +
  # Add a vertical line at the grand mean
  geom_vline(
    xintercept = mean(pull(results_all_df, p_hat)), 
    linetype = "dashed", 
    color = "red"
  ) +
  labs(
    title = "Estimated Unsolved Homicide Proportion by City",
    subtitle = "95% Confidence Intervals",
    x = "Proportion of Unsolved Homicides",
    y = "City, State"
    ) +
  scale_x_continuous(labels = scales::percent) +
  theme(
    axis.text.y = element_text(size = 7),
    panel.grid.major.y = element_line(linetype = "dotted", color = "gray90")
  )

unsolved_proportions_plot
```

<img src="p8105_hw5_ls4236_files/figure-gfm/unnamed-chunk-15-1.png" width="90%" />

- The plot shows the estimated proportion of unsolved homicides across
  numerous U.S. cities, sorted from the highest to the lowest clearance
  rates from bottom to the top.

- The data reveals a wide disparity in homicide clearance, ranging from
  cities like Tulsa, AL (where the data only includes one solved case,
  no unsolved case) to Chicago, IL, where the estimated proportion of
  unsolved cases is highest, exceeding 70%.

- The confidence intervals demonstrate that while estimates for cities
  with large volumes of homicides (like Chicago) are precise (narrow
  bars), the precision varies across the board, and the grand average
  unsolved proportion (red dashed line) suggests that many cities
  struggle to solve even half of their homicide cases.
